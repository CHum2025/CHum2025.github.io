---
layout: single
title: Program
permalink: /program.html
classes: wide
---

<div class="page__content">

  <p>The following schedule is tentative and subject to change.</p>

  <p>All times are in <strong><a href="https://www.timeanddate.com/time/zones/gst">Gulf Standard Time (UTC+04:00)</a></strong>.</p>

  <p>All presentations will take place via Zoom; connection details will be provided to registered participants.</p>

<h2></h2>

<table class="program">
<tr class="program-header"><td colspan="2">Saturday, January 19, 2025</td></tr>
<tr><td>08:45–09:00</td><td>Opening remarks<br>Christian F. Hempelmann, Julia Rayz, Tiansi Dong, and Tristan Miller</td></tr>
<tr class="program-keynote"><td colspan="2">Keynote #1</td></tr>
<tr><td>09:00–09:45</td><td>How Many Stochastic Parrots Does it Take to Change a Lightbulb?<br>Salvatore Attardo, Texas A&amp;M University–Commerce<br>
<span class="abstract">Generally speaking the discussion of humor and AI has centered on questions such as “Does AI have a sense of humor?” or “Does AI understand humor and/or chan it explain it?” or “Can AI produce humor?” More pragmatic projects  tend to focus on recognition: “Can AI identify humor/irony/puns/satire?”
I find these questions interesting, but up to a point. First, the seem all tinged, more or less consciously, to what we could call the “anthropomorphic AI ideology”: the belief that AIs match/reflect/approximate human intelligence and that humor is a folk-metric of performance, a sort of Humorous Turing test: if AIs understand/produce humor then they are truly “human” (for some senses of “be” and “human” to be specified). The character of Data in <emph>Star Trek: The Next Generation</emph> is the standard bearer for this set of beliefs. Second, the idea, to which I shamefully contributed, that humor is “AI complete” and hence if an AI “masters” humor it should thereby be an AGI (Artificial General Intelligence). Third, the current wave of LLMs are essentially black boxes and it is unclear to what extent a black box model counts as an explanation.
Having managed to alienate the entire audience of the gathering, I will then proceed to argue that LLMs provide us with an excellent implementation of Trier’s (1931) lexical field theory and more broadly of the semantic network postulated by Fillmore, Raskin, Eco and many others. As such they make some aspects of theories of humor testable in a new empirical way.</span></td></tr>
<tr class="program-session"><td colspan="2">Paper session #1: Multimodal Pathways to Meaning and Memory</td></tr>
<tr><td>09:45–10:05</td><td>"The Exception of Humor: Iconicity, Phonemic Surprisal, Memory Recall, and Emotional Associations"<br>Alexander Kilpatrick and Maria Flaksman</td></tr>
<tr><td>10:05–10:25</tD><td>"Text Is Not All You Need: Multimodal Prompting Helps LLMs Understand Humor"<br>Ashwin Baluja</td></tr>
<tr class="program-break"><td colspan="2">Coffee break</td></tr>
<tr class="program-keynote"><td colspan="2">Keynote #2</td></tr>
<tr><td>11:00–11:45</td><td>Unlocking the Punchline: Navigating Humor Computation From Understanding to Generation and Beyond<br>Liang Yang, Dalian University of Technology<br>
<span class="abstract">Significant progress has been made in the field of humor computation, including recognition and generation. Despite its promising developments, several challenges still remain in the era of LLMs. In this talk, I will present our recent efforts to address some of them. (1)&nbsp;For understanding, I will discuss the understanding mechanism of large models for humor. (2)&nbsp;For generation, I will introduce how large models generate specific types of humor by integrating humor theory. (3)&nbsp;Finally, I will share our future exploration directions, such evaluation of sense of humor.</span></td></tr>
<tr class="program-session"><td colspan="2">Paper session #2: Algorithms and Frameworks for Pun Generation</td></tr>
<tr><td>11:45–12:05</td><td>"Rule-based Approaches to the Automatic Generation of Puns Based on Given Names in French"<br>Mathieu Dehouck and Marine Delaborde</td></tr>
<tr><td>12:05–12:25</td><td>"Homophonic Pun Generation in Code Mixed Hindi English"<br>Yash Raj Sarrof</td></tr>
<tr><td>12:25–12:45</td><td>"Bridging Laughter Across Languages: Generation of Hindi–English Codemixed Puns"<br><span class="presenter">Likhith Asapu</span>, Prashant Kodali, Ashna Dua, Kapil Rajesh Kavitha, and Manish Shrivastava</td></tr>
<tr class="program-break"><td colspan="2">Lunch break</td></tr>
<tr class="program-keynote"><td colspan="2">Keynote #3</td></tr>
<tr><td>13:30–14:15</td><td>Title TBA<br>Willibald Ruch, University of Zurich</td></tr>
<tr class="program-session"><td colspan="2">Paper session #3: Explorations in Semantics and Pragmatics</td></tr>
<tr><td>14:15–14:35</td><td>"Testing Humor Theory Using Word and Sentence Embeddings"<br><span class="presenter">Stephen Skalicky</span> and Salvatore Attardo</td></tr>
<tr><td>14:35–14:55</td><td>"Pragmatic Metacognitive Prompting Improves LLM Performance on Sarcasm Detection"<br>Joshua Lee, Wyatt Fong, Alexander Le, Sur Shah, Kevin Han, and Kevin Zhu</td></tr>
<tr class="program-keynote"><td colspan="2">Keynote #4</td></tr>
<tr><td>14:55–15:40</td><td> Get With The Program! From Talking the Talk to Walking the Walk in Computational Humour<br>Tony Veale, University College Dublin<br>
<span class="abstract">LLM-based agents such as ChatGPT exhibit an amiable if superficial wit, and show a skilled ear for mimicry that lets them capture the cadences and attitudes of famous comics. Their auto-regressive generation of continuations to a given prompt makes them especially adept at using the “<emph>yes, and …</emph>” principle of improv comedy to wring laughs from silly premises. But jokes of the short and snappy variety that are crafted to be retold in many different contexts require an LLM to do more than talk like a comedian. The words and attitudes must be appropriate to a joke, and the text must actually work like a joke too, with an effective logical mechanism that snaps shut on an audience like a mouse in a trap. Although LLMs can do a surprisingly good job of analyzing and explaining jokes, they do a rather poor job of turning this reactive appreciation into a proactive generative ability. In this talk I consider how we might tackle this <emph>understanding vs. generating</emph> gap. </span></td></tr>
<tr class="program-break"><td colspan="2">Coffee break</td></tr>
<tr class="program-session"><td colspan="2">Paper session #4: Evaluating and Experiencing Generated Humor</td></tr>
<tr><td>16:10–16:30</td><td>"Can AI Make Us Laugh? Comparing Jokes Generated by Witscript and a Human Expert"<br>Joe Toplyn and Ori Amir</td></tr>
<tr><td>16:30–16:50</td><td>"Evaluating Human Perception and Bias in AI-Generated Humor"<br>Narendra Nath Joshi</td></tr>
<tr><td>16:50–17:10</td><td>"The Theater Stage as Laboratory: Review of Real-Time Comedy LLM Systems for Live Performance"<br>Piotr Mirowski, Kory Mathewson, and Boyd Branch</td></tr>
<tr><td>17:10–17:30</td><td>"The Algorithm is the Message: Computing as a Humor-Generating Mode"<br>Vittorio Marone</td></tr>
<tr><td>17:30–17:40</td><td>Closing remarks<br>Christian F. Hempelmann, Julia Rayz, Tiansi Dong, and Tristan Miller</td></tr>
</table>

</div>


